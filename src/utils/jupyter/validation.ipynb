{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb55d8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jiwer --quiet\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#os.chdir(\"../../../\")\n",
    "%load_ext sql\n",
    "conn = duckdb.connect(\"duckdb/main.duckdb\")\n",
    "write_conn = duckdb.connect(\"duckdb/main.duckdb\")  # For UPDATEs and COPY\n",
    "%sql conn --alias duckdb\n",
    "\n",
    "\n",
    "%config SqlMagic.displaylimit = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafe308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select  * FROm block_quality where correct_ind IN('Y','C') and len(newspaper) < 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d20eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "/* Check for newspapers that do not have text from current model */\n",
    "select count(*) from(\n",
    "SELECT PV.Newspaper, PV.Block , PV.Text\n",
    "FROM Full_Text PV\n",
    "LEFT JOIN Full_Text NV\n",
    "ON NV.Newspaper = PV.Newspaper\n",
    "AND NV.Block = PV.Block\n",
    "AND NV.Source = 'gpt-4o'\n",
    "AND NV.Type = 'block_prompt'\n",
    "LEFT JOIN Full_Text NVGem\n",
    "ON NVGem.Newspaper = PV.Newspaper\n",
    "AND NVGem.Block = PV.Block\n",
    "AND NVGem.Source = 'gemini-2.5-pro'\n",
    "AND NVGem.Type = 'block_prompt'\n",
    "WHERE NV.Text IS NULL\n",
    "AND NVGem IS NULL\n",
    "AND PV.Source = 'split_blocks.py'\n",
    "AND PV.Type = 'N/A'\n",
    "AND len(PV.Text) > 100 \n",
    "AND NOT EXISTS(\n",
    "SELECT 1 FROM BLOCK_QUALITY BQ\n",
    "WHERE BQ.NEwspaper = PV.newspaper\n",
    "AND left(BQ.block,7) = PV.block\n",
    "AND Bq.Correct_Ind  = 'Y'\n",
    " )) a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b342784",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()\n",
    "write_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e854c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "--SELECT newspaper, idx, CER, WER, Per_word_error FROM val_text\n",
    "\n",
    "select Newspaper, CAST(mean(cer) AS DECIMAL(18,2)) CER , CAST(mean(wer) AS DECIMAL(18,2)) WER, CAST(mean(per_word_error) AS DECIMAL(18,2))BoW from val_text \n",
    "\n",
    "--where newspaper <>  '3B_Model_Plus1000.json' \n",
    "group by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82622162",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select * from full_text where\n",
    "1=1\n",
    "--AND block = 'TB00012'\n",
    "--AND Newspaper = 'postimeesew19371212.1.2'\n",
    "and  text like '%Nõnda et annetustena on tulnud hinge pealt%' \n",
    "--and block <> 'All'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0b9a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "select 'Newspapers' as Type, Count(*) as Amt FROM(select distinct newspaper FROM full_text where source = 'dea.digar')A \n",
    "union all\n",
    "select 'Text_Length' as Type, sum(text) as Amt FROM(select len(text) as text FROM full_text where source = 'dea.digar' )A \n",
    "union all\n",
    "select 'Final text' , sum(ln) FROM(select len(train_input) - 8 /* The coordinates in the end */ as ln  FROM OLMOCR_train ) a\n",
    "union all\n",
    "select 'Final Blocks' , count(*) FROM(select distinct newspaper,block FROM OLMOCR_train ) a\n",
    "union all\n",
    " select 'Final Newspapers' , count(*) FROM(select distinct newspaper FROM OLMOCR_train ) a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b456ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SElect count(*)  FROM full_text ft\n",
    "where source IN('split_blocks_no_coords')\n",
    "and not exists(\n",
    "    select 1 from OLMOCR_train OT\n",
    "    where ot.newspaper = ft.newspaper\n",
    ")\n",
    "--and len(text) > 100\n",
    "limit 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb27bcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select *From OLMOCR_train where newspaper ='sakalaew19361118.1.6' and block = 'TB00004'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e677ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "--select count(*) FROM(select distinct newspaper, block FROM full_text where Source = 'gemini-2.5-pro' and Type = 'block_prompt')\n",
    "select count(*) FROM(select distinct newspaper, block FROM full_text where Source = 'gemini-2.5-pro' and Type = 'block_prompt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8584f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%sql\n",
    "select count(*) FROM(\n",
    "SELECT PV.Newspaper, PV.Block , PV.Text, nvgem.*\n",
    "FROM Full_Text PV\n",
    "LEFT JOIN Full_Text NV\n",
    "ON NV.Newspaper = PV.Newspaper\n",
    "AND NV.Block = PV.Block\n",
    "AND NV.Source = 'gpt-4o'\n",
    "AND NV.Type = 'block_prompt'\n",
    "LEFT JOIN Full_Text NVGem\n",
    "ON NVGem.Newspaper = PV.Newspaper\n",
    "AND NVGem.Block = PV.Block\n",
    "AND NVGem.Source = 'gemini-2.5-pro'\n",
    "AND NVGem.Type = 'block_prompt'\n",
    "\n",
    "WHERE \n",
    "1=1\n",
    "AND NV.Text IS NULL\n",
    "AND NVGem.Text IS NULL\n",
    "AND PV.Source = 'split_blocks.py'\n",
    "AND PV.Type = 'N/A'\n",
    "AND len(PV.Text) > 100 \n",
    "AND  NOT EXISTS(\n",
    "SELECT 1 FROM BLOCK_QUALITY BQ\n",
    "WHERE BQ.NEwspaper = PV.newspaper\n",
    "AND left(BQ.block,7) = PV.block\n",
    "AND Bq.Correct_Ind = 'Y' \n",
    ") )a\n",
    "--and pv.block = 'TB00001' and pv.newspaper = 'eestikirikeelk19260527.1.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6111c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select count(*) FROM(\n",
    "\n",
    "SELECT \n",
    "Newspaper,\n",
    "Block,\n",
    "MAX(CASE WHEN Source = 'split_blocks_pypdf' then trim(text) else null END)  AS Train_Input,\n",
    "MAX(CASE WHEN Source IN( 'split_blocks_no_coords') then trim(text) else null END)  AS Split_blocks_No_Coords\n",
    "FROM Full_Text\n",
    "where source  IN( 'split_blocks_pypdf','split_blocks_no_coords')\n",
    "AND Block <> 'All'\n",
    "AND EXISTS(\n",
    "SELECT 1 FROM BLOCK_QUALITY BQ\n",
    "WHERE BQ.NEwspaper = Full_Text.newspaper\n",
    "AND left(BQ.block,7) = full_text.block\n",
    "AND Bq.Correct_Ind = 'Y' \n",
    ")\n",
    "GROUP BY Newspaper,Block\n",
    "having Train_input is not null and Split_blocks_No_Coords is not null\n",
    " ) a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ddbb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT distinct newspaper  FROM val_text-- where newspaper = '3B_Model_Plus1000.json'  and cer >= 0.9 order by cer desc limit 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74dc31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "select \n",
    "Newspaper,\n",
    "sum(case when cer <= 0.01 then 1 else 0 end ) GOOD,\n",
    "max(case when cer <= 0.01 then len(reference) - len(response) else null end ) GOOD_LEN_DIFF,\n",
    "avg(case when cer <= 0.01 then len(reference) END ) AS good_len_avg,\n",
    "sum(case when cer <= 0.05 and cer > 0.01  then 1 else 0 end ) OK,\n",
    "max(case when cer <= 0.05 and cer > 0.01  then len(reference) - len(response) else null end ) OK_LEN_DIFF,\n",
    "avg(case when cer <= 0.05 and cer > 0.01 then len(reference) END ) AS ok_len_avg,\n",
    "sum(case when cer > 0.05 then 1 else 0 end ) NOK,\n",
    "max(case when cer > 0.05 then len(reference) - len(response) else null end ) NOK_LEN_DIFF,\n",
    "mean(case when cer > 0.05 then len(reference) END ) AS nok_len_avg,\n",
    "sum(case when cer >= 0.5 then 1 else 0 end ) BAD,\n",
    "max(case when cer >= 0.5  then len(reference) - len(response) else null end ) BAD_LEN_DIFF,\n",
    "mean(case when cer >= 0.5 then len(reference) END ) AS bad_len_avg,\n",
    "sum(case when cer >= 0.9 then 1 else 0 end ) Horrible,\n",
    "max(case when cer >= 0.9 then len(reference) - len(response) else null end ) HORRIBLE_LEN_DIFF,\n",
    "mean(case when cer >= 0.9 then len(reference) END ) AS horrible_len_avg\n",
    "\n",
    "\n",
    " from val_Text \n",
    " where newspaper = '3B_Model_Plus1000.json' \n",
    " group by 1\n",
    "  limit 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7030e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "select *FROM(\n",
    "select \n",
    "CASE WHEN Newspaper = 'QWEN_3B_Base.json' THEN 'Base ' || Category ELSE 'Tuned ' ||  Category end AS Category,\n",
    "Count as Count,\n",
    "CAST(abs(Len_Diff) AS  INTEGER) Len_Diff ,\n",
    "CAST(Len_Avg AS DECIMAL(18,1)) Len_Avg\n",
    "FROM(\n",
    "\n",
    "SELECT \n",
    "       Newspaper,\n",
    "       '<1%' AS category, \n",
    "        1 rnk,\n",
    "       SUM(CASE WHEN cer <= 0.01 THEN 1 ELSE 0 END) AS count,\n",
    "       MAX(CASE WHEN cer <= 0.01 THEN len(reference) - len(response) ELSE NULL END) AS len_diff,\n",
    "       AVG(CASE WHEN cer <= 0.01 THEN len(reference) END) AS len_avg\n",
    "FROM val_Text\n",
    "WHERE Newspaper IN('QWEN_3B_Base.json','3B_Model.json') \n",
    "GROUP BY Newspaper\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT\n",
    "       Newspaper,\n",
    " '<5%' AS category, \n",
    "        2,\n",
    "       SUM(CASE WHEN cer <= 0.05 AND cer > 0.01 THEN 1 ELSE 0 END) AS count,\n",
    "       MAX(CASE WHEN cer <= 0.05 AND cer > 0.01 THEN len(reference) - len(response) ELSE NULL END) AS len_diff,\n",
    "       AVG(CASE WHEN cer <= 0.05 AND cer > 0.01 THEN len(reference) END) AS len_avg\n",
    "FROM val_Text\n",
    "WHERE Newspaper IN('QWEN_3B_Base.json','3B_Model.json') \n",
    "GROUP BY Newspaper\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "       Newspaper,\n",
    "'>5%' AS category, \n",
    "3   ,\n",
    "       SUM(CASE WHEN cer > 0.05 AND cer < 0.5  THEN 1 ELSE 0 END) AS count,\n",
    "       MAX(CASE WHEN cer > 0.05 AND cer < 0.5 THEN len(reference) - len(response) ELSE NULL END) AS len_diff,\n",
    "       AVG(CASE WHEN cer > 0.05 AND cer < 0.5 THEN len(reference) END) AS len_avg\n",
    "FROM val_Text\n",
    "WHERE Newspaper IN('QWEN_3B_Base.json','3B_Model.json') \n",
    "GROUP BY Newspaper\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "       Newspaper,\n",
    "'>50%' AS category, \n",
    "   4,\n",
    "       SUM(CASE WHEN cer >= 0.5 AND cer < 0.9 THEN 1 ELSE 0 END) AS count,\n",
    "       MAX(CASE WHEN cer >= 0.5 AND cer < 0.9 THEN len(reference) - len(response) ELSE NULL END) AS len_diff,\n",
    "       AVG(CASE WHEN cer >= 0.5 AND cer < 0.9 THEN len(reference) END) AS len_avg\n",
    "FROM val_Text\n",
    "WHERE Newspaper IN('QWEN_3B_Base.json','3B_Model.json') \n",
    "GROUP BY Newspaper\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT\n",
    "       Newspaper,\n",
    " '>90%' AS category, \n",
    "5,\n",
    "       SUM(CASE WHEN cer >= 0.9 THEN 1 ELSE 0 END) AS count,\n",
    "       MAX(CASE WHEN cer >= 0.9 THEN len(reference) - len(response) ELSE NULL END) AS len_diff,\n",
    "       AVG(CASE WHEN cer >= 0.9 THEN len(reference) END) AS len_avg\n",
    "FROM val_Text\n",
    "WHERE Newspaper IN('QWEN_3B_Base.json','3B_Model.json') \n",
    "GROUP BY Newspaper) a  order by rnk asc) a order by category asc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375f1bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()\n",
    "write_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d89254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_root = \"src/datasets/silver/scanned\"\n",
    "\n",
    "for paper_name in os.listdir(silver_root):\n",
    "    paper_path = os.path.join(silver_root, paper_name)\n",
    "    textfile_path = os.path.join(paper_path, \"text\", \"continuous_text.txt\")\n",
    "    if not os.path.isfile(textfile_path):\n",
    "        continue\n",
    "\n",
    "    with open(textfile_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        textfile_content = \"\".join(f.readlines())\n",
    "\n",
    "    # Insert only if this newspaper doesn't already exist with the same source/type\n",
    "    write_conn.execute(\"\"\"\n",
    "        INSERT INTO Full_Text (Newspaper, Source, Type, Text, Timestamp)\n",
    "        SELECT ?, 'dea.digar', 'corrected', ?, CURRENT_TIMESTAMP\n",
    "        WHERE NOT EXISTS (\n",
    "            SELECT 1 FROM Full_Text\n",
    "            WHERE Newspaper = ?\n",
    "              AND Source = 'dea.digar'\n",
    "              AND Type = 'corrected'\n",
    "        )\n",
    "        \"\"\", (paper_name, textfile_content, paper_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1799d564",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT *   FROM Full_Text\n",
    "where\n",
    "1=1\n",
    "AND newspaper = 'sakalaew19361118.1.6'\n",
    "and Source = 'dea.digar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59a3934",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE TABLE IF NOT EXISTS val_text (\n",
    "    Newspaper TEXT,\n",
    "    Source TEXT,\n",
    "    Type TEXT,\n",
    "    CER DOUBLE,\n",
    "    WER DOUBLE,\n",
    "    Per_Word_Error INTEGER,  \n",
    "    Timestamp TIMESTAMP\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7abf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_conn.execute(\"\"\"\n",
    "        COPY Validation TO 'duckdb/validation.parquet' (FORMAT PARQUET);\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacbc21a",
   "metadata": {},
   "source": [
    "# LLM diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860989d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "from jiwer import cer\n",
    "import re\n",
    "from collections import Counter\n",
    "def normalize(text):\n",
    "    # Fix hyphenation at end of line (e.g., \"soo-\\nmaline\" → \"soomaline\")\n",
    "    text = re.sub(r'-\\s*\\n', '', text)\n",
    "\n",
    "    # Replace \" - \" or other mid-sentence hyphens with space (e.g., \"miski - muu\" → \"miski muu\")\n",
    "    text = re.sub(r'\\s*[-–—]\\s*', ' ', text)\n",
    "\n",
    "    # Normalize w → v\n",
    "    text = text.replace('w', 'v').replace('W', 'V')\n",
    "\n",
    "    # Remove multiple newlines and whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def count_words(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return Counter(text.split())\n",
    "\n",
    "def dict_diff(dict1, dict2):\n",
    "    diff = {}\n",
    "    # Keys only in dict1\n",
    "    for k in dict1:\n",
    "        if k not in dict2:\n",
    "            diff[k] = ('only_in_orig', dict1[k])\n",
    "        elif dict1[k] != dict2[k]:\n",
    "            diff[k] = ('different_values', dict1[k], dict2[k])\n",
    "    # Keys only in dict2\n",
    "    for k in dict2:\n",
    "        if k not in dict1:\n",
    "            diff[k] = ('only_in_new', dict2[k])\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8aeb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "from jiwer import cer\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def normalize_ocr_text(text):\n",
    "    # Step 1: Remove hyphenation across lines (e.g. wa-\\nrastati -> warastati)\n",
    "    text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text)\n",
    "\n",
    "    # Step 2: Replace newlines with spaces\n",
    "    text = re.sub(r'\\s*\\n\\s*', ' ', text)\n",
    "\n",
    "    # Step 3: Normalize spacing\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "cursor = conn.execute(\n",
    "    \"\"\"\n",
    "SELECT \n",
    "Newspaper,\n",
    "Block,\n",
    "MAX(CASE WHEN Source IN( 'split_blocks_no_coords' )then trim(text) else null END)  AS Manual,\n",
    "MAX(CASE WHEN Source IN( 'gemini-2.5-pro') then trim(text) else null END)  AS Pro,\n",
    "MAX(CASE WHEN Source IN( 'gpt-4o') then trim(text) else null END)  AS GPT,\n",
    "MAX(CASE WHEN Source IN( 'split_blocks_pypdf_no_coord' )then trim(text) else null END) AS Pypdf\n",
    "FROM Full_Text\n",
    "where source  IN( 'gemini-2.5-pro','gpt-4o','split_blocks_no_coords','split_blocks_pypdf_no_coord')\n",
    "AND Block <> 'All'\n",
    "AND  EXISTS(\n",
    "SELECT 1 FROM BLOCK_QUALITY BQ\n",
    "WHERE BQ.NEwspaper = Full_Text.newspaper\n",
    "AND left(BQ.block,7) = full_text.block\n",
    "AND Bq.Correct_Ind = 'Y' \n",
    ")\n",
    "GROUP BY Newspaper,Block\n",
    "having len(gpt) > 0  and  len(pro) > 0 is not null \n",
    "\"\"\")\n",
    "\n",
    "row = cursor.fetchone()\n",
    "results = []\n",
    "\n",
    "\n",
    "while row:\n",
    "    newspaper,block,orig, pro, gpt, pypdf = row\n",
    "    \n",
    "    # Remove some unimportant difference, might not be needed for untrained data\n",
    "    #original_digar = normalize(orig) \n",
    "    #pro_norm = normalize(pro) \n",
    "    #gpt_norm = normalize(gpt)\n",
    "    original_digar = orig\n",
    "    pro_norm = pro\n",
    "    gpt_norm = gpt\n",
    "    pypdf_norm = pypdf\n",
    "\n",
    "    if not pypdf_norm:\n",
    "        errorc_pypdf = 9999\n",
    "        errorc_pypdf_modern = 9999\n",
    "    if pypdf_norm:\n",
    "        modified_pypder = pypdf_norm.replace(\"w\", \"v\").replace(\"W\", \"V\")\n",
    "        modified_pypdf = normalize_ocr_text(pypdf_norm)\n",
    "        errorc_pypdf = cer(original_digar, pypdf_norm)\n",
    "        errorc_pypdf_modern = cer(modified_orig, modified_pypdf)\n",
    "\n",
    "    # Modernize w -> v\n",
    "    modified_gpt= gpt_norm.replace(\"w\", \"v\").replace(\"W\", \"V\")\n",
    "    modified_pro= pro_norm.replace(\"w\", \"v\").replace(\"W\", \"V\")\n",
    "    modified_orig = original_digar.replace(\"w\", \"v\").replace(\"W\", \"V\")\n",
    "    \n",
    "    \n",
    "    modified_gpt = normalize_ocr_text(modified_gpt)\n",
    "    modified_pro = normalize_ocr_text(modified_pro)\n",
    "    modified_orig = normalize_ocr_text(modified_orig)\n",
    "\n",
    "    \n",
    "    errorc_pro = cer(original_digar, pro_norm)\n",
    "    errorc_gpt = cer(original_digar, gpt_norm)\n",
    "    \n",
    "    errorc_pro_modern = cer(modified_orig, modified_pro)\n",
    "    errorc_gpt_modern = cer(modified_orig, modified_gpt)\n",
    "    #print(errorc_pro)\n",
    "    #print(errorc_gpt)\n",
    "    \n",
    "\n",
    "\n",
    "    df = pd.DataFrame ([{\n",
    "            \"newspaper\": newspaper,\n",
    "            \"block\" : block,\n",
    "            \"orig\": original_digar,\n",
    "            \"Gemini\": pro_norm,\n",
    "            \"GPT\": gpt_norm,\n",
    "            \"cer_pro\": errorc_pro,\n",
    "            \"ceer_gpt\": errorc_gpt,\n",
    "            \"ceer_pydpf\": errorc_pypdf,\n",
    "            \"ceer_mod_gpt\": errorc_gpt_modern,\n",
    "            \"ceer_mod_pro\": errorc_pro_modern,\n",
    "            \"ceer_mod_pypdf\" : errorc_pypdf_modern,\n",
    "        }])\n",
    "\n",
    "    results.append({\n",
    "        \"newspaper\": newspaper,\n",
    "        \"block\": block,\n",
    "        \"cer_pro\": errorc_pro,\n",
    "        \"cer_gpt\": errorc_gpt,\n",
    "        \"ceer_pydpf\": errorc_pypdf,\n",
    "        \"ceer_mod_gpt\": errorc_gpt_modern,\n",
    "        \"ceer_mod_pro\": errorc_pro_modern,\n",
    "        \"ceer_mod_pypdf\" : errorc_pypdf_modern,\n",
    "        \"orig\": original_digar,\n",
    "        \"Gemini\": pro_norm,\n",
    "        \"GPT\": gpt_norm,\n",
    "        \"pypdf\": pypdf_norm\n",
    "    })\n",
    "    \n",
    "    row = cursor.fetchone()\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e48bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_conn.register(\"df\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074ecaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT \n",
    "sum(case when  ceer_mod_pro >= 0.1 then 1 else 0 end ) mt_01,\n",
    "sum(case when  ceer_mod_pro > 0.05 and ceer_mod_pro < 0.1 then 1 else 0 end ) mt_005,\n",
    "sum(case when  ceer_mod_pro <= 0.1 then 1 else 0 end ) lt_005,\n",
    "sum(case when  ceer_mod_gpt >= 0.1 then 1 else 0 end ) mt_01,\n",
    "sum(case when  ceer_mod_gpt > 0.05 and ceer_mod_gpt < 0.1 then 1 else 0 end ) mt_005,\n",
    "sum(case when  ceer_mod_gpt <= 0.1 then 1 else 0 end ) lt_005,\n",
    "avg(cer_pro),\n",
    "avg(cer_gpt),\n",
    "avg(ceer_mod_pro),\n",
    "avg(ceer_mod_gpt)\n",
    "    from df --order by cer_diff desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcd352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "select\n",
    "--regexp_extract(newspaper, '^(.*?)(?=\\d{8})') AS publication,\n",
    "newspaper,\n",
    "CAST(mean(ceer_pydpf) AS DECIMAL(18,2)) as unm_ceer\n",
    "--,mean(ceer_mod_pypdf)   as m_ceer\n",
    "\n",
    "FROM df where ceer_pydpf <> 9999 and ceer_mod_pypdf <> 9999\n",
    "group by newspaper\n",
    "order by unm_ceer desc\n",
    "\n",
    "--where ceer_pydpf > 1.0 limit 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6862030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select * From Full_Text where source ='split_blocks_pypdf_no_coord'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f75f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select count(*) FROM(\n",
    "SELECT \n",
    "case when ceer_mod_pro <= 0.05 and ceer_mod_gpt >  0.05 then 1 else 0 end  as GPT_Negative,\n",
    "case when ceer_mod_pro > 0.05 and ceer_mod_gpt <=  0.05 then 1 else 0 end  as Pro_Negative,\n",
    "--ceer_mod_pro,\n",
    "--ceer_mod_gpt,\n",
    "--abs(ceer_mod_gpt - ceer_mod_pro) as diff,\n",
    "--df.*\n",
    "    from df\n",
    "  where  \n",
    " -- ((ceer_mod_pro <= 0.05 and ceer_mod_gpt >  0.05 ) OR (ceer_mod_pro > 0.05 and ceer_mod_gpt <=  0.05))\n",
    "\n",
    "\n",
    "   (ceer_mod_pro <= 0.05 OR ceer_mod_gpt <= 0.05)\n",
    "  )a\n",
    "  --   order by diff desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a185c1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select *  FROM full_text where source IN('gemini-2.5-pro','gpt-4o','split_blocks_no_coords','split_blocks.py','dea.digar') and newspaper ='kaja19350917-1.1.7' and (block = 'TB00046' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb97d5d6",
   "metadata": {},
   "source": [
    "## LLM diff no community indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec3067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "from jiwer import cer\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def normalize_ocr_text(text):\n",
    "    # Step 1: Remove hyphenation across lines (e.g. wa-\\nrastati -> warastati)\n",
    "    text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text)\n",
    "\n",
    "    # Step 2: Replace newlines with spaces\n",
    "    text = re.sub(r'\\s*\\n\\s*', ' ', text)\n",
    "\n",
    "    # Step 3: Normalize spacing\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "cursor = conn.execute(\n",
    "    \"\"\"\n",
    "SELECT \n",
    "Newspaper,\n",
    "Block,\n",
    "MAX(CASE WHEN Source IN( 'split_blocks_no_coords' )then trim(text) else null END)  AS Manual,\n",
    "MAX(CASE WHEN Source IN( 'gemini-2.5-pro') then trim(text) else null END)  AS Pro,\n",
    "MAX(CASE WHEN Source IN( 'gpt-4o') then trim(text) else null END)  AS GPT\n",
    "FROM Full_Text\n",
    "where source  IN( 'gemini-2.5-pro','gpt-4o','split_blocks_no_coords')\n",
    "AND Block <> 'All'\n",
    "AND  NOT EXISTS(\n",
    "SELECT 1 FROM BLOCK_QUALITY BQ\n",
    "WHERE BQ.NEwspaper = Full_Text.newspaper\n",
    "AND left(BQ.block,7) = full_text.block\n",
    "AND Bq.Correct_Ind = 'Y' \n",
    ")\n",
    "GROUP BY Newspaper,Block\n",
    "HAVING (gpt IS NOT NULL AND length(gpt) > 0) OR (pro IS NOT NULL AND length(pro) > 0)\n",
    "\"\"\")\n",
    "\n",
    "row = cursor.fetchone()\n",
    "results = []\n",
    "\n",
    "\n",
    "while row:\n",
    "    newspaper,block,orig, pro, gpt = row\n",
    "    \n",
    "    # Remove some unimportant difference, might not be needed for untrained data\n",
    "    #original_digar = normalize(orig) \n",
    "    #pro_norm = normalize(pro) \n",
    "    #gpt_norm = normalize(gpt)\n",
    "    gpt_norm = \"\"\n",
    "    pro_norm = \"\"\n",
    "    original_digar = orig\n",
    "    modified_orig = original_digar.replace(\"w\", \"v\").replace(\"W\", \"V\")\n",
    "    modified_orig = normalize_ocr_text(modified_orig)\n",
    "    if not pro:\n",
    "        errorc_pro = 9999\n",
    "        errorc_pro_modern = 9999\n",
    "    else:\n",
    "        pro_norm = pro\n",
    "        modified_pro= pro_norm.replace(\"w\", \"v\").replace(\"W\", \"V\")\n",
    "        modified_pro = normalize_ocr_text(modified_pro)\n",
    "        errorc_pro = cer(original_digar, pro_norm)\n",
    "        errorc_pro_modern = cer(modified_orig, modified_pro)\n",
    "    if not gpt:\n",
    "        errorc_gpt = 9999\n",
    "        errorc_gpt_modern = 9999\n",
    "    \n",
    "    else:\n",
    "        gpt_norm = gpt\n",
    "    # Modernize w -> v\n",
    "        modified_gpt= gpt_norm.replace(\"w\", \"v\").replace(\"W\", \"V\")\n",
    "    \n",
    "        modified_gpt = normalize_ocr_text(modified_gpt)\n",
    "        errorc_gpt = cer(original_digar, gpt_norm)\n",
    "        errorc_gpt_modern = cer(modified_orig, modified_gpt)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(errorc_pro)\n",
    "    #print(errorc_gpt)\n",
    "    \n",
    "\n",
    "\n",
    "    df = pd.DataFrame ([{\n",
    "            \"newspaper\": newspaper,\n",
    "            \"block\" : block,\n",
    "            \"orig\": original_digar,\n",
    "            \"Gemini\": pro_norm,\n",
    "            \"GPT\": gpt_norm,\n",
    "            \"cer_pro\": errorc_pro,\n",
    "            \"ceer_gpt\": errorc_gpt,\n",
    "            \"ceer_mod_gpt\": errorc_gpt_modern,\n",
    "            \"ceer_mod_pro\": errorc_pro_modern,\n",
    "        }])\n",
    "\n",
    "    results.append({\n",
    "        \"newspaper\": newspaper,\n",
    "        \"block\": block,\n",
    "        \"cer_pro\": errorc_pro,\n",
    "        \"cer_gpt\": errorc_gpt,\n",
    "        \"ceer_mod_gpt\": errorc_gpt_modern,\n",
    "        \"ceer_mod_pro\": errorc_pro_modern,\n",
    "        \"orig\": original_digar,\n",
    "        \"Gemini\": pro_norm,\n",
    "        \"GPT\": gpt_norm\n",
    "    })\n",
    "    \n",
    "    row = cursor.fetchone()\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "write_conn.register(\"df\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d0148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select * FROM block_quality where correct_ind IN( 'C','Y') and newspaper ='kaja19290813-1.1.5' and block = 'TB00006.xml'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5be599",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT newspaper, block, count(*)\n",
    " FROM block_quality ft where correct_ind IN( 'C','Y')\n",
    "\n",
    "\n",
    " and line_amt >  1\n",
    "  group by  1,2 having count(*) > 1\n",
    "  --group by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049b3638",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select \n",
    "*\n",
    "--distinct block\n",
    " --source ,count(*) knt \n",
    "FROM full_text where newspaper ='postimeesew19390716.1.1' \n",
    "--and source = 'anchoring.py'\n",
    "--group by 1 order by knt desc\n",
    "and block not like '%TB00%'\n",
    "--and block ='TB00004.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9195352",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select *  FROM\n",
    "block_quality where correct_ind IN( 'C','Y')\n",
    "\n",
    "\n",
    "and (newspaper, left(block,7)) not in\n",
    "(\n",
    "SELECT \n",
    "Newspaper,\n",
    "Block\n",
    "--Train_Input,\n",
    "--case when pro is not null then 'Pro' Else 'FLASH' END AS Ind,\n",
    "--COALESCE(Pro,FLash) AS Train_Output\n",
    "--Split_blocks_No_Coords as Train_Output\n",
    "FROM(\n",
    "SELECT \n",
    "Newspaper,\n",
    "Block,\n",
    "MAX(CASE WHEN Source = 'split_blocks_pypdf' then trim(text) else null END)  AS Train_Input,\n",
    "MAX(CASE WHEN Source IN( 'split_blocks_no_coords') then trim(text) else null END)  AS Split_blocks_No_Coords\n",
    "FROM Full_Text\n",
    "where source  IN( 'split_blocks_pypdf','split_blocks_no_coords')\n",
    "--AND Block <> 'All'\n",
    "AND NOT EXISTS(\n",
    "    SELECT 1 FROM OLMOCR_TRAIN \n",
    "    WHERE OLMOCR_TRAIN.Newspaper = Full_text.newspaper\n",
    "    AND OLMOCR_TRAIN.BLock = FUll_Text.BLock\n",
    ")\n",
    "/*AND EXISTS(\n",
    "SELECT 1 FROM BLOCK_QUALITY BQ\n",
    "WHERE BQ.NEwspaper = Full_Text.newspaper\n",
    "AND left(BQ.block,7) = full_text.block\n",
    "AND Bq.Correct_Ind = 'Y' \n",
    ")*/\n",
    "GROUP BY Newspaper,Block\n",
    "having Train_input is not null and Split_blocks_No_Coords is not null\n",
    ") SQ --  WHERE len(Train_Output) > 0 and len(Train_input) > 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caf2529",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "--INSERT INTO block_quality (Newspaper, Block,Correct_ind, Line_amt )\n",
    "select  \n",
    "Newspaper, Block, 'C', 0 \n",
    "FROM(\n",
    "SELECT \n",
    "Newspaper, Block,ceer_mod_pro,ceer_mod_gpt\n",
    "--case when ceer_mod_pro <= 0.05 and ceer_mod_gpt >  0.05 then 1 else 0 end  as GPT_Negative,\n",
    "--case when ceer_mod_pro > 0.05 and ceer_mod_gpt <=  0.05 then 1 else 0 end  as Pro_Negative,\n",
    "--ceer_mod_pro,\n",
    "--ceer_mod_gpt,\n",
    "--abs(ceer_mod_gpt - ceer_mod_pro) as diff,\n",
    "--df.*\n",
    "    from df\n",
    "  where  \n",
    "-- ((ceer_mod_pro <= 0.05 and ceer_mod_gpt >  0.05 ) OR (ceer_mod_pro > 0.05 and ceer_mod_gpt <=  0.05))\n",
    "\n",
    "\n",
    "   (ceer_mod_pro <= 0.05 OR ceer_mod_gpt <= 0.05)\n",
    "  )a\n",
    "  --   order by diff desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d22a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT  * \n",
    "FROM Full_Text\n",
    "WHERE\n",
    "1=1\n",
    "--AND timestamp > current_timestamp - INTERVAL 4 HOUR\n",
    "--and Source = 'gpt-4o'\n",
    "AND newspaper = 'postimeesew19370426.1.1'\n",
    "and block = 'TB00048'\n",
    "limit 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c9d83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "from jiwer import cer\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def normalize(text):\n",
    "    # Fix hyphenation at end of line (e.g., \"soo-\\nmaline\" → \"soomaline\")\n",
    "    text = re.sub(r'-\\s*\\n', '', text)\n",
    "\n",
    "    # Replace \" - \" or other mid-sentence hyphens with space (e.g., \"miski - muu\" → \"miski muu\")\n",
    "    text = re.sub(r'\\s*[-–—]\\s*', ' ', text)\n",
    "\n",
    "    # Normalize w → v\n",
    "    text = text.replace('w', 'v').replace('W', 'V')\n",
    "\n",
    "    # Remove multiple newlines and whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def count_words(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return Counter(text.split())\n",
    "\n",
    "def dict_diff(dict1, dict2):\n",
    "    diff = {}\n",
    "    # Keys only in dict1\n",
    "    for k in dict1:\n",
    "        if k not in dict2:\n",
    "            diff[k] = ('only_in_orig', dict1[k])\n",
    "        elif dict1[k] != dict2[k]:\n",
    "            diff[k] = ('different_values', dict1[k], dict2[k])\n",
    "    # Keys only in dict2\n",
    "    for k in dict2:\n",
    "        if k not in dict1:\n",
    "            diff[k] = ('only_in_new', dict2[k])\n",
    "    return diff\n",
    "\n",
    "\n",
    "cursor = conn.execute(\n",
    "    f\"\"\"\n",
    "/* Check for newspapers that do not have text from current model */\n",
    "SELECT digar.Newspaper, other.Source, other.Type, state.Correct_Percent ,digar.Text AS digar_text, other.Text AS other_text\n",
    "FROM Full_Text digar\n",
    "LEFT JOIN Full_Text other\n",
    "ON digar.Newspaper = other.Newspaper\n",
    "AND other.Source <> 'dea.digar'\n",
    "AND LEN(other.Text) > 10\n",
    "LEFT JOIN state\n",
    " ON digar.Newspaper = state.Newspaper\n",
    "WHERE digar.Source = 'dea.digar'\n",
    "AND digar.Type = 'corrected'\n",
    "and other.Source IS NOT NULL\n",
    "and other.Source = 'split_blocks_pypdf'\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "row = cursor.fetchone()\n",
    "\n",
    "while row :\n",
    "    # Original text has all w turned to v and removed all dashes.\n",
    "    # Detected text needs to have all end of lines removed with the same treatment as original text\n",
    "\n",
    "    newspaper = row[0]\n",
    "    source = row[1]\n",
    "    typedesc = row[2]\n",
    "    correct_percent = row[3]\n",
    "    digar_text = row[4]\n",
    "    other_text = row[5]\n",
    "\n",
    "    if not all([newspaper, source, typedesc, digar_text, other_text]):\n",
    "        print(f\"Skipping row due to missing values: {row}\")\n",
    "        row = cursor.fetchone()\n",
    "        continue\n",
    "\n",
    "    original_digar = normalize(digar_text) # Text from 'dea.digar'\n",
    "    new_input = normalize(other_text) # Text from the other source\n",
    "\n",
    "\n",
    "\n",
    "    errorw = wer(original_digar, new_input)\n",
    "    errorc = cer(original_digar, new_input)\n",
    "    word_count_orig = count_words(original_digar)\n",
    "    word_count_new = count_words(new_input)\n",
    "    per_word_error = round(len(dict_diff(word_count_orig, word_count_new)) / len(word_count_orig) * 100, 1)\n",
    "\n",
    "\n",
    "    #print(f\"Newspaper: {row[0]}\")\n",
    "    #print(f\"Initial quality: {row[3]}%\")\n",
    "    #print(f\"Error rate by word count: {per_word_error}%\")\n",
    "    #print(f\"Word Error Rate (WER): {errorw}\")\n",
    "    #print(f\"Character Error Rate (CER): {errorc}\")\n",
    "    #print(dict_diff(word_count_orig,word_count_new))\n",
    "    #print(original_digar)\n",
    "    #print(\"---\")\n",
    "    #print(new_input)\n",
    "    #print(row[5])\n",
    "    write_conn.execute(\"\"\"\n",
    "        INSERT INTO Validation (Newspaper, Source,Type,CER,WER,Per_Word_Error, Timestamp )\n",
    "        SELECT ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP\n",
    "        WHERE NOT EXISTS (\n",
    "            SELECT 1 FROM Validation\n",
    "            WHERE Newspaper = ?\n",
    "                  AND Source = ?\n",
    "                  AND Type = ?\n",
    "\n",
    "        )\n",
    "        \"\"\", (newspaper, source, typedesc,errorc,errorw,per_word_error,newspaper,source, typedesc))\n",
    "    \n",
    "    row = cursor.fetchone()\n",
    "\n",
    "write_conn.execute(\"\"\"\n",
    "        COPY Validation TO 'duckdb/validation.parquet' (FORMAT PARQUET);\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a682f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "select * FROM Validation where Source = 'split_blocks_pypdf' limit 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0dbf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute(\"SELECT Text FROM Full_Text WHERE Newspaper='sakalaew19361118.1.6' and  source = 'anchoring.py'\").fetchone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc37d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT \n",
    "*\n",
    "--Source,\n",
    "--max(Text,1)\n",
    " FROM Full_Text\n",
    "WHERE Newspaper= 'sakalaew19361118.1.6'\n",
    "and source IN( 'split_blocks.py','split_blocks_no_coords')\n",
    "and block ='TB00004'\n",
    "--and text like '%õisu%'\n",
    "--group by Source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb2cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.anchor_olmocr import get_anchor_text\n",
    "\n",
    "anchor_text = get_anchor_text(\"/mnt/3de36453-6164-4568-91b5-ae973509273e/Git/EE-Gothic-Script-OCR/src/datasets/bronze/scanned/sakalaew19361118.1.6/image/sakalaew19361118.1.6.pdf\", 1, pdf_engine=\"pdfreport\")\n",
    "\n",
    "jpg_width = 2479\n",
    "jpg_height = 3508\n",
    "\n",
    "row_nbr = 0\n",
    "\n",
    "lines = []\n",
    "\n",
    "for row in anchor_text.splitlines():\n",
    "    if row_nbr < 2:\n",
    "        if row_nbr == 0:\n",
    "            page_size_list = row.split(\" \")\n",
    "\n",
    "            x_split = page_size_list[-1].split(\"x\")\n",
    "\n",
    "            pdf_width = float(x_split[0])\n",
    "            pdf_height  = float(x_split[1])\n",
    "\n",
    "            row_nbr += 1\n",
    "        else:\n",
    "            row_nbr += 1\n",
    "    elif row.startswith('['):\n",
    "        #print(row)\n",
    "        ct_split = row.split(']')\n",
    "        #print(ct_split[1])\n",
    "        coord = ct_split[0]  # Get text before the first ']'\n",
    "        text = ct_split[1]\n",
    "        text = text.strip()\n",
    "        #print(text)\n",
    "        coord = coord.strip('[')   # Remove the '[' from the start\n",
    "        coord = coord.split('x') # Output: 12.34, 56.78 etc.\n",
    "\n",
    "        #x_img = (x_pdf / pdf_width) * img_width\n",
    "        #_img = img_height - ((y_pdf / pdf_height) * img_height)\n",
    "        x_img = int(float(coord[0]) / pdf_width *   jpg_width)\n",
    "        y_img = int(jpg_height - (float(coord[1]) / pdf_height * jpg_height) )\n",
    "        line = f\"{text} [{x_img},{y_img}] \\n \"\n",
    "        lines.append(line)\n",
    "\n",
    "block = ''.join(lines)\n",
    "print(block)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atm_2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
