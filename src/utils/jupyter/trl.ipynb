{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143a8ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020-2025 The HuggingFace Team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# /// script\n",
    "# dependencies = [\n",
    "#     \"trl @ git+https://github.com/huggingface/trl.git\",\n",
    "#     \"Pillow>=9.4.0\",\n",
    "# ]\n",
    "# ///\n",
    "\n",
    "\"\"\"\n",
    "pip install pillow\n",
    "\n",
    "# Tested on 8x H100 GPUs\n",
    "accelerate launch\n",
    "    --config_file=examples/accelerate_configs/deepspeed_zero3.yaml \\\n",
    "    examples/scripts/sft_vlm.py \\\n",
    "    --dataset_name HuggingFaceH4/llava-instruct-mix-vsft \\\n",
    "    --model_name_or_path llava-hf/llava-1.5-7b-hf \\\n",
    "    --per_device_train_batch_size 8 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --output_dir sft-llava-1.5-7b-hf \\\n",
    "    --bf16 \\\n",
    "    --torch_dtype bfloat16 \\\n",
    "    --gradient_checkpointing\n",
    "\n",
    "For LLaVA-NeXT, use: (requires transformers>=4.45)\n",
    "    --model_name_or_path llava-hf/llava-v1.6-mistral-7b-hf\n",
    "\n",
    "For meta-llama/Llama-3.2-11B-Vision-Instruct, use: (requires transformers>=4.45.1)\n",
    "    --model_name_or_path meta-llama/Llama-3.2-11B-Vision-Instruct\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, LlavaForConditionalGeneration\n",
    "import os\n",
    "from PIL import Image\n",
    "import io\n",
    "import json\n",
    "from transformers import EarlyStoppingCallback\n",
    "import trl \n",
    "                             \n",
    "\n",
    "from trl import (\n",
    "    ModelConfig,\n",
    "    ScriptArguments,\n",
    "    SFTConfig,\n",
    "    SFTTrainer,\n",
    "    TrlParser,\n",
    "    get_kbit_device_map,\n",
    "    get_peft_config,\n",
    "    get_quantization_config,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "error_cnt  = 0 \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = TrlParser((ScriptArguments, SFTConfig, ModelConfig))\n",
    "    script_args, training_args, model_args = parser.parse_args_and_config(args=[])\n",
    "\n",
    "    training_args.gradient_checkpointing_kwargs = dict(use_reentrant=False)\n",
    "    training_args.remove_unused_columns = False\n",
    "    training_args.dataset_kwargs = {\"skip_prepare_dataset\": True}\n",
    "\n",
    "    ################\n",
    "    # Model, Tokenizer & Processor\n",
    "    ################\n",
    "    torch_dtype = (\n",
    "        model_args.torch_dtype if model_args.torch_dtype in [\"auto\", None] else getattr(torch, model_args.torch_dtype)\n",
    "    )\n",
    "    quantization_config = get_quantization_config(model_args)\n",
    "    model_kwargs = dict(\n",
    "        revision=model_args.model_revision,\n",
    "        attn_implementation=model_args.attn_implementation,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device_map=get_kbit_device_map() if quantization_config is not None else None,\n",
    "        quantization_config=quantization_config,\n",
    "    )\n",
    "    processor = AutoProcessor.from_pretrained(\n",
    "        model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code\n",
    "    )\n",
    "\n",
    "    model = AutoModelForVision2Seq.from_pretrained(\n",
    "        model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code, **model_kwargs\n",
    "    )\n",
    "\n",
    "    ################\n",
    "    # Create a data collator to encode text and image pairs\n",
    "    ################\n",
    "    RESPONSE_MARKER = \"<|startofassistant|>\"\n",
    "\n",
    "    def extract_image(example):\n",
    "        try:\n",
    "            messages = example[\"messages\"]\n",
    "            if isinstance(messages, str):\n",
    "                messages = json.loads(messages) \n",
    "\n",
    "            for message in messages:\n",
    "                if message[\"role\"] == \"user\":\n",
    "                    for item in message[\"content\"]:\n",
    "                        if item[\"type\"] == \"image\":\n",
    "                            img_data = item[\"image\"]\n",
    "                            # Handle the image data (path, PIL.Image, or dict with bytes)\n",
    "                            if isinstance(img_data, dict) and \"bytes\" in img_data:\n",
    "                                img = Image.open(io.BytesIO(img_data[\"bytes\"])).convert(\"RGB\")\n",
    "                                return img\n",
    "                            elif isinstance(img_data, str):\n",
    "                                img = Image.open(img_data).convert(\"RGB\")\n",
    "                                return img\n",
    "                            elif hasattr(img_data, \"size\"):\n",
    "                                return img_data\n",
    "                            else:\n",
    "                                return None\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting image: {e}\")\n",
    "            return None\n",
    "\n",
    "    def filter_long_examples(example):\n",
    "        try:\n",
    "            messages = deserialize_messages(example[\"messages\"])\n",
    "            if hasattr(processor.tokenizer, \"chat_template\") and processor.tokenizer.chat_template:\n",
    "                prompt = processor.apply_chat_template(messages, tokenize=False)\n",
    "            else:\n",
    "                prompt = \"\\n\".join(f\"<|{m['role']}|> {m['content']}\" for m in messages)\n",
    "\n",
    "            tokenized = processor.tokenizer(prompt, truncation=False, padding=False)\n",
    "            return len(tokenized[\"input_ids\"]) <= 2100\n",
    "        except Exception as e:\n",
    "            print(f\"Filter error: {e}\")\n",
    "            return False  # Fail-safe: filter out if any error\n",
    "        \n",
    "    def deserialize_messages(messages):\n",
    "        if isinstance(messages, str):\n",
    "            return json.loads(messages)\n",
    "        return messages\n",
    "        \n",
    "    def truncate_chat_template(template, max_chars=6000):\n",
    "        if len(template) <= max_chars:\n",
    "            return template\n",
    "        return template[:max_chars]\n",
    "\n",
    "    def collate_fn(examples):\n",
    "    \n",
    "        messages_batch = []\n",
    "        for example in examples:\n",
    "            messages = deserialize_messages(example[\"messages\"])\n",
    "            for m in messages:\n",
    "                if m[\"role\"] == \"assistant\" and isinstance(m[\"content\"], list) and len(m[\"content\"]) == 1:\n",
    "                    m[\"content\"][0][\"text\"] = RESPONSE_MARKER + m[\"content\"][0][\"text\"]\n",
    "            if hasattr(processor.tokenizer, \"chat_template\") and processor.tokenizer.chat_template:\n",
    "                prompt = processor.apply_chat_template(messages, tokenize=False)\n",
    "            else:\n",
    "                prompt = \"\\n\".join(f\"<|{m['role']}|> {m['content']}\" for m in messages)\n",
    "\n",
    "            if RESPONSE_MARKER in prompt:\n",
    "                messages_batch.append(prompt)\n",
    "            else:\n",
    "                error_cnt += 1\n",
    "\n",
    "        if len(messages_batch) == 0:\n",
    "            return None\n",
    "\n",
    "        images = [extract_image(example) for example in examples]\n",
    "\n",
    "        batch = processor(images=images, text=messages_batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "        input_lengths = batch[\"input_ids\"].ne(processor.tokenizer.pad_token_id).sum(dim=1)\n",
    "        # Truncate token-level to max 8192 tokens\n",
    "        max_length = 8192\n",
    "        batch[\"input_ids\"] = batch[\"input_ids\"][:, :max_length]\n",
    "        batch[\"attention_mask\"] = batch[\"attention_mask\"][:, :max_length]\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # Find response marker in each sequence\n",
    "        marker_token_ids = processor.tokenizer.encode(RESPONSE_MARKER, add_special_tokens=False)\n",
    "        marker_len = len(marker_token_ids)\n",
    "        \n",
    "\n",
    "        \n",
    "        for i, ids in enumerate(input_ids):\n",
    "        # Try to find the marker\n",
    "            marker_found = False\n",
    "            for j in range(len(ids) - marker_len + 1):\n",
    "                if torch.equal(ids[j:j+marker_len], torch.tensor(marker_token_ids, device=ids.device)):\n",
    "                    labels[i, :j+marker_len] = -100  # Mask everything before and including the marker\n",
    "                    marker_found = True\n",
    "                    break\n",
    "        \n",
    "            if not marker_found:\n",
    "                # If marker not found, mask everything\n",
    "                labels[i] = -100\n",
    "                #print(f\"Warning: Response marker not found in sequence {i}\")\n",
    "\n",
    "        \n",
    "        print(batch)\n",
    "        return batch\n",
    "\n",
    "\n",
    "    ################\n",
    "    # Dataset\n",
    "    ################\n",
    "    dataset = load_from_disk(\"/mnt/3de36453-6164-4568-91b5-ae973509273e/Git/EE-Gothic-Script-OCR/src/datasets/gold/block_dataset\")\n",
    "\n",
    "    # Filter the dataset\n",
    "    dataset = dataset.filter(filter_long_examples)\n",
    "    eval_dataset_full = dataset[script_args.dataset_test_split]\n",
    "    #eval_dataset = eval_dataset_full.select(range(min(5, len(eval_dataset_full))))\n",
    "    eval_dataset = eval_dataset_full\n",
    "\n",
    "    ################\n",
    "    # Training\n",
    "    ################\n",
    "    training_args.metric_for_best_model = \"eval_loss\"\n",
    "    training_args.load_best_model_at_end = True\n",
    "\n",
    "    #trainer = SFTTrainer(\n",
    "    #    model=model,\n",
    "    #    args=training_args,\n",
    "    #    data_collator=collate_fn,\n",
    "    #    train_dataset=dataset[script_args.dataset_train_split],\n",
    "    #    eval_dataset=eval_dataset if training_args.eval_strategy != \"no\" else None,\n",
    "    #    processing_class=processor,\n",
    "    #    peft_config=get_peft_config(model_args),\n",
    "    #    \n",
    "    #    \n",
    "    #)\n",
    "    #\n",
    "    #trainer.add_callback(EarlyStoppingCallback(\n",
    "    #early_stopping_patience=3,\n",
    "    #early_stopping_threshold=0.001))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atm_2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
